{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trains = (50, 100)\n",
    "num_tests = (50, 100, 200, 500)\n",
    "accuracies_home_dir: str = \"accuracies_from_paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for num_train in num_trains:\n",
    "    for num_test in num_tests:\n",
    "        dfs.append(\n",
    "            utils.load_all_accuracies(\n",
    "                os.path.join(accuracies_home_dir, f\"m{num_train}\"), num_test\n",
    "            ).with_columns(\n",
    "                pl.lit(num_train).alias(\"num_train\"), pl.lit(num_test).alias(\"num_test\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df: pl.DataFrame = pl.concat(dfs)\n",
    "\n",
    "group = [\"num_train\", \"num_test\", \"lm_type\", \"dataset\"]\n",
    "accuracies_df = accuracies_df.sort(by=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_df = accuracies_df.with_columns(\n",
    "    pretraining_boost=pl.col(\"extra\") - pl.col(\"base\"),\n",
    "    evaluation_bias=pl.col(\"test\") - pl.col(\"extra\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sem(col: pl.Expr) -> pl.Expr:\n",
    "    # ty https://github.com/pola-rs/polars/issues/6175#issuecomment-1416846104\n",
    "    return col.std() / (col.count().sqrt())\n",
    "\n",
    "\n",
    "accuracies_grouped = (  # aggregate subsamples into mean and SE\n",
    "    accuracies_df.group_by(*group)\n",
    "    .agg(\n",
    "        pl.col(\"pretraining_boost\").mean().name.suffix(\"_mean\"),\n",
    "        sem(pl.col(\"pretraining_boost\")).name.suffix(\"_se\"),\n",
    "        pl.col(\"evaluation_bias\").mean().name.suffix(\"_mean\"),\n",
    "        sem(pl.col(\"evaluation_bias\")).name.suffix(\"_se\"),\n",
    "    )\n",
    "    .sort(group)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (400, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num_train</th><th>num_test</th><th>lm_type</th><th>dataset</th><th>pretraining_boost_mean</th><th>pretraining_boost_se</th><th>evaluation_bias_mean</th><th>evaluation_bias_se</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>50</td><td>50</td><td>&quot;bert&quot;</td><td>&quot;FRENK-hate-en&quot;</td><td>-0.0264</td><td>0.009228</td><td>-0.0008</td><td>0.009551</td></tr><tr><td>50</td><td>50</td><td>&quot;bert&quot;</td><td>&quot;ag_news&quot;</td><td>0.1006</td><td>0.0098</td><td>-0.0162</td><td>0.011223</td></tr><tr><td>50</td><td>50</td><td>&quot;bert&quot;</td><td>&quot;amazon_counter…</td><td>0.0198</td><td>0.018692</td><td>0.0232</td><td>0.020512</td></tr><tr><td>50</td><td>50</td><td>&quot;bert&quot;</td><td>&quot;app_reviews&quot;</td><td>0.1186</td><td>0.012834</td><td>0.007</td><td>0.015615</td></tr><tr><td>50</td><td>50</td><td>&quot;bert&quot;</td><td>&quot;blog_authorshi…</td><td>-0.006</td><td>0.007001</td><td>0.0064</td><td>0.007638</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>100</td><td>500</td><td>&quot;gpt2&quot;</td><td>&quot;silicone&quot;</td><td>0.0352</td><td>0.009397</td><td>-0.0095</td><td>0.005943</td></tr><tr><td>100</td><td>500</td><td>&quot;gpt2&quot;</td><td>&quot;trec&quot;</td><td>0.0214</td><td>0.009417</td><td>-0.0009</td><td>0.00403</td></tr><tr><td>100</td><td>500</td><td>&quot;gpt2&quot;</td><td>&quot;tweets_hate_sp…</td><td>0.0233</td><td>0.027941</td><td>-0.0056</td><td>0.019901</td></tr><tr><td>100</td><td>500</td><td>&quot;gpt2&quot;</td><td>&quot;yahoo_answers_…</td><td>-0.0109</td><td>0.004161</td><td>-0.0015</td><td>0.002384</td></tr><tr><td>100</td><td>500</td><td>&quot;gpt2&quot;</td><td>&quot;yelp_review_fu…</td><td>-0.0355</td><td>0.007782</td><td>0.0014</td><td>0.002974</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (400, 8)\n",
       "┌───────────┬──────────┬─────────┬─────────────┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ num_train ┆ num_test ┆ lm_type ┆ dataset     ┆ pretrainin ┆ pretrainin ┆ evaluation ┆ evaluation │\n",
       "│ ---       ┆ ---      ┆ ---     ┆ ---         ┆ g_boost_me ┆ g_boost_se ┆ _bias_mean ┆ _bias_se   │\n",
       "│ i32       ┆ i32      ┆ str     ┆ str         ┆ an         ┆ ---        ┆ ---        ┆ ---        │\n",
       "│           ┆          ┆         ┆             ┆ ---        ┆ f64        ┆ f64        ┆ f64        │\n",
       "│           ┆          ┆         ┆             ┆ f64        ┆            ┆            ┆            │\n",
       "╞═══════════╪══════════╪═════════╪═════════════╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 50        ┆ 50       ┆ bert    ┆ FRENK-hate- ┆ -0.0264    ┆ 0.009228   ┆ -0.0008    ┆ 0.009551   │\n",
       "│           ┆          ┆         ┆ en          ┆            ┆            ┆            ┆            │\n",
       "│ 50        ┆ 50       ┆ bert    ┆ ag_news     ┆ 0.1006     ┆ 0.0098     ┆ -0.0162    ┆ 0.011223   │\n",
       "│ 50        ┆ 50       ┆ bert    ┆ amazon_coun ┆ 0.0198     ┆ 0.018692   ┆ 0.0232     ┆ 0.020512   │\n",
       "│           ┆          ┆         ┆ terfactual_ ┆            ┆            ┆            ┆            │\n",
       "│           ┆          ┆         ┆ en          ┆            ┆            ┆            ┆            │\n",
       "│ 50        ┆ 50       ┆ bert    ┆ app_reviews ┆ 0.1186     ┆ 0.012834   ┆ 0.007      ┆ 0.015615   │\n",
       "│ 50        ┆ 50       ┆ bert    ┆ blog_author ┆ -0.006     ┆ 0.007001   ┆ 0.0064     ┆ 0.007638   │\n",
       "│           ┆          ┆         ┆ ship_corpus ┆            ┆            ┆            ┆            │\n",
       "│ …         ┆ …        ┆ …       ┆ …           ┆ …          ┆ …          ┆ …          ┆ …          │\n",
       "│ 100       ┆ 500      ┆ gpt2    ┆ silicone    ┆ 0.0352     ┆ 0.009397   ┆ -0.0095    ┆ 0.005943   │\n",
       "│ 100       ┆ 500      ┆ gpt2    ┆ trec        ┆ 0.0214     ┆ 0.009417   ┆ -0.0009    ┆ 0.00403    │\n",
       "│ 100       ┆ 500      ┆ gpt2    ┆ tweets_hate ┆ 0.0233     ┆ 0.027941   ┆ -0.0056    ┆ 0.019901   │\n",
       "│           ┆          ┆         ┆ _speech_det ┆            ┆            ┆            ┆            │\n",
       "│           ┆          ┆         ┆ ection      ┆            ┆            ┆            ┆            │\n",
       "│ 100       ┆ 500      ┆ gpt2    ┆ yahoo_answe ┆ -0.0109    ┆ 0.004161   ┆ -0.0015    ┆ 0.002384   │\n",
       "│           ┆          ┆         ┆ rs_topics   ┆            ┆            ┆            ┆            │\n",
       "│ 100       ┆ 500      ┆ gpt2    ┆ yelp_review ┆ -0.0355    ┆ 0.007782   ┆ 0.0014     ┆ 0.002974   │\n",
       "│           ┆          ┆         ┆ _full       ┆            ┆            ┆            ┆            │\n",
       "└───────────┴──────────┴─────────┴─────────────┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bias = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (14, 6)\n",
      "┌───────────┬──────────┬─────────┬────────────────────────────────┬──────────────────────┬────────────────────┐\n",
      "│ num_train ┆ num_test ┆ lm_type ┆ dataset                        ┆ evaluation_bias_mean ┆ evaluation_bias_se │\n",
      "│ ---       ┆ ---      ┆ ---     ┆ ---                            ┆ ---                  ┆ ---                │\n",
      "│ i32       ┆ i32      ┆ str     ┆ str                            ┆ f64                  ┆ f64                │\n",
      "╞═══════════╪══════════╪═════════╪════════════════════════════════╪══════════════════════╪════════════════════╡\n",
      "│ 50        ┆ 50       ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.0232               ┆ 0.020512           │\n",
      "│ 50        ┆ 50       ┆ bert    ┆ trec                           ┆ 0.0238               ┆ 0.015268           │\n",
      "│ 50        ┆ 200      ┆ bert    ┆ mtop_domain                    ┆ 0.0318               ┆ 0.018115           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.0504               ┆ 0.033422           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ clickbait_notclickbait_dataset ┆ 0.0259               ┆ 0.025745           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ emo                            ┆ 0.0519               ┆ 0.03033            │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ emotion                        ┆ 0.0202               ┆ 0.024272           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ financial_phrasebank           ┆ 0.0424               ┆ 0.025981           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ silicone                       ┆ 0.0224               ┆ 0.033429           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.027                ┆ 0.022846           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ craigslist_bargains            ┆ 0.044                ┆ 0.029942           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ mtop_domain                    ┆ 0.0301               ┆ 0.023935           │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.0581               ┆ 0.036525           │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ trec                           ┆ 0.036                ┆ 0.029784           │\n",
      "└───────────┴──────────┴─────────┴────────────────────────────────┴──────────────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1, tbl_width_chars=1_000):\n",
    "    print(\n",
    "        accuracies_grouped.filter(pl.col(\"evaluation_bias_mean\") > big_bias)\n",
    "        .sort(group)\n",
    "        .select(group + [\"evaluation_bias_mean\", \"evaluation_bias_se\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 1)\n",
      "┌────────────────────────────────┐\n",
      "│ dataset                        │\n",
      "│ ---                            │\n",
      "│ str                            │\n",
      "╞════════════════════════════════╡\n",
      "│ amazon_counterfactual_en       │\n",
      "│ clickbait_notclickbait_dataset │\n",
      "│ craigslist_bargains            │\n",
      "│ emo                            │\n",
      "│ emotion                        │\n",
      "│ financial_phrasebank           │\n",
      "│ mtop_domain                    │\n",
      "│ silicone                       │\n",
      "│ trec                           │\n",
      "└────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_width_chars=1_000):\n",
    "    print(\n",
    "        accuracies_grouped.filter(pl.col(\"evaluation_bias_mean\") > big_bias)[\"dataset\"]\n",
    "        .unique()\n",
    "        .sort()\n",
    "        .to_frame()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (14, 6)\n",
      "┌───────────┬──────────┬─────────┬────────────────────────────────┬──────────────────────┬────────────────────┐\n",
      "│ num_train ┆ num_test ┆ lm_type ┆ dataset                        ┆ evaluation_bias_mean ┆ evaluation_bias_se │\n",
      "│ ---       ┆ ---      ┆ ---     ┆ ---                            ┆ ---                  ┆ ---                │\n",
      "│ i32       ┆ i32      ┆ str     ┆ str                            ┆ f64                  ┆ f64                │\n",
      "╞═══════════╪══════════╪═════════╪════════════════════════════════╪══════════════════════╪════════════════════╡\n",
      "│ 50        ┆ 50       ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.0232               ┆ 0.020512           │\n",
      "│ 50        ┆ 50       ┆ bert    ┆ trec                           ┆ 0.0238               ┆ 0.015268           │\n",
      "│ 50        ┆ 200      ┆ bert    ┆ mtop_domain                    ┆ 0.0318               ┆ 0.018115           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.0504               ┆ 0.033422           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ clickbait_notclickbait_dataset ┆ 0.0259               ┆ 0.025745           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ emo                            ┆ 0.0519               ┆ 0.03033            │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ emotion                        ┆ 0.0202               ┆ 0.024272           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ financial_phrasebank           ┆ 0.0424               ┆ 0.025981           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ silicone                       ┆ 0.0224               ┆ 0.033429           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.027                ┆ 0.022846           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ craigslist_bargains            ┆ 0.044                ┆ 0.029942           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ mtop_domain                    ┆ 0.0301               ┆ 0.023935           │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ amazon_counterfactual_en       ┆ 0.0581               ┆ 0.036525           │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ trec                           ┆ 0.036                ┆ 0.029784           │\n",
      "└───────────┴──────────┴─────────┴────────────────────────────────┴──────────────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1, tbl_width_chars=1_000):\n",
    "    print(\n",
    "        accuracies_grouped.filter(pl.col(\"evaluation_bias_mean\") > big_bias)\n",
    "        .sort(group)\n",
    "        .select(group + [\"evaluation_bias_mean\", \"evaluation_bias_se\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty sure they're all `bert` b/c it has much higher variance. Can test it out by\n",
    "seeing which LM types pop up when reversing the direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (11, 6)\n",
      "┌───────────┬──────────┬─────────┬──────────────────────────────┬──────────────────────┬────────────────────┐\n",
      "│ num_train ┆ num_test ┆ lm_type ┆ dataset                      ┆ evaluation_bias_mean ┆ evaluation_bias_se │\n",
      "│ ---       ┆ ---      ┆ ---     ┆ ---                          ┆ ---                  ┆ ---                │\n",
      "│ i32       ┆ i32      ┆ str     ┆ str                          ┆ f64                  ┆ f64                │\n",
      "╞═══════════╪══════════╪═════════╪══════════════════════════════╪══════════════════════╪════════════════════╡\n",
      "│ 50        ┆ 200      ┆ bert    ┆ silicone                     ┆ -0.0217              ┆ 0.019896           │\n",
      "│ 50        ┆ 200      ┆ bert    ┆ tweets_hate_speech_detection ┆ -0.0593              ┆ 0.024333           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ hyperpartisan_news_detection ┆ -0.0272              ┆ 0.010065           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ mtop_domain                  ┆ -0.0355              ┆ 0.021482           │\n",
      "│ 50        ┆ 500      ┆ bert    ┆ tweets_hate_speech_detection ┆ -0.0639              ┆ 0.04675            │\n",
      "│ 100       ┆ 100      ┆ bert    ┆ climate_fever                ┆ -0.0236              ┆ 0.012205           │\n",
      "│ 100       ┆ 100      ┆ bert    ┆ craigslist_bargains          ┆ -0.0371              ┆ 0.016537           │\n",
      "│ 100       ┆ 200      ┆ bert    ┆ emo                          ┆ -0.022               ┆ 0.020348           │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ mtop_domain                  ┆ -0.0512              ┆ 0.03989            │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ rotten_tomatoes              ┆ -0.0492              ┆ 0.023742           │\n",
      "│ 100       ┆ 500      ┆ bert    ┆ silicone                     ┆ -0.0306              ┆ 0.020533           │\n",
      "└───────────┴──────────┴─────────┴──────────────────────────────┴──────────────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1, tbl_width_chars=1_000):\n",
    "    print(\n",
    "        accuracies_grouped.filter(pl.col(\"evaluation_bias_mean\") < -big_bias)\n",
    "        .sort(group)\n",
    "        .select(group + [\"evaluation_bias_mean\", \"evaluation_bias_se\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (25, 3)\n",
      "┌────────────────────────────────┬────────────────────────┬──────────────────────┐\n",
      "│ dataset                        ┆ pretraining_boost_mean ┆ evaluation_bias_mean │\n",
      "│ ---                            ┆ ---                    ┆ ---                  │\n",
      "│ str                            ┆ f64                    ┆ f64                  │\n",
      "╞════════════════════════════════╪════════════════════════╪══════════════════════╡\n",
      "│ mtop_domain                    ┆ 0.142169               ┆ -0.003463            │\n",
      "│ app_reviews                    ┆ 0.1134125              ┆ 0.002575             │\n",
      "│ emo                            ┆ 0.098594               ┆ -0.0018              │\n",
      "│ tweets_hate_speech_detection   ┆ 0.081575               ┆ -0.008256            │\n",
      "│ amazon_counterfactual_en       ┆ 0.078344               ┆ 0.009306             │\n",
      "│ ag_news                        ┆ 0.078325               ┆ -0.001613            │\n",
      "│ silicone                       ┆ 0.049681               ┆ -0.001281            │\n",
      "│ emotion                        ┆ 0.04825                ┆ 0.001088             │\n",
      "│ rotten_tomatoes                ┆ 0.047838               ┆ -0.002794            │\n",
      "│ hyperpartisan_news_detection   ┆ 0.0432                 ┆ -0.003187            │\n",
      "│ patent-classification          ┆ 0.0426                 ┆ -0.001013            │\n",
      "│ trec                           ┆ 0.039475               ┆ 0.002819             │\n",
      "│ massive                        ┆ 0.03855                ┆ -0.001819            │\n",
      "│ enron_spam                     ┆ 0.038281               ┆ -0.001775            │\n",
      "│ yelp_review_full               ┆ 0.031975               ┆ -0.001463            │\n",
      "│ clickbait_notclickbait_dataset ┆ 0.0297625              ┆ 0.0023625            │\n",
      "│ yahoo_answers_topics           ┆ 0.026413               ┆ -0.000225            │\n",
      "│ disaster_response_messages     ┆ 0.023369               ┆ 0.000713             │\n",
      "│ FRENK-hate-en                  ┆ 0.0188875              ┆ -0.000869            │\n",
      "│ climate_fever                  ┆ 0.008844               ┆ 0.001088             │\n",
      "│ limit                          ┆ 0.00835                ┆ 0.000987             │\n",
      "│ financial_phrasebank           ┆ 0.007894               ┆ 0.007406             │\n",
      "│ craigslist_bargains            ┆ 0.006769               ┆ 0.0014               │\n",
      "│ movie_rationales               ┆ -0.004675              ┆ 0.000381             │\n",
      "│ blog_authorship_corpus         ┆ -0.009588              ┆ 0.001025             │\n",
      "└────────────────────────────────┴────────────────────────┴──────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1):\n",
    "    print(\n",
    "        accuracies_grouped.group_by(\"dataset\")\n",
    "        .agg(\n",
    "            pl.col(\"pretraining_boost_mean\").mean(),\n",
    "            pl.col(\"evaluation_bias_mean\").mean(),\n",
    "        )\n",
    "        .sort(\"pretraining_boost_mean\", descending=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "┌────────────────────────┬────────────────────────┬─────────────────┐\n",
      "│ dataset                ┆ pretraining_boost_mean ┆ frac_boost_lt_0 │\n",
      "│ ---                    ┆ ---                    ┆ ---             │\n",
      "│ str                    ┆ f64                    ┆ f64             │\n",
      "╞════════════════════════╪════════════════════════╪═════════════════╡\n",
      "│ blog_authorship_corpus ┆ -0.009588              ┆ 0.625           │\n",
      "│ movie_rationales       ┆ -0.004675              ┆ 0.6875          │\n",
      "└────────────────────────┴────────────────────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1):\n",
    "    print(\n",
    "        accuracies_grouped.group_by(\"dataset\")\n",
    "        .agg(  # across m, n, lm_type\n",
    "            pl.col(\"pretraining_boost_mean\").mean(),\n",
    "            (pl.col(\"pretraining_boost_mean\") < 0).mean().alias(\"frac_boost_lt_0\"),\n",
    "        )\n",
    "        .filter(pl.col(\"pretraining_boost_mean\") < 0)\n",
    "        .sort(\"pretraining_boost_mean\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (12, 3)\n",
      "┌────────────────────────────────┬──────────────────────┬────────────────┐\n",
      "│ dataset                        ┆ evaluation_bias_mean ┆ frac_bias_gt_0 │\n",
      "│ ---                            ┆ ---                  ┆ ---            │\n",
      "│ str                            ┆ f64                  ┆ f64            │\n",
      "╞════════════════════════════════╪══════════════════════╪════════════════╡\n",
      "│ amazon_counterfactual_en       ┆ 0.009306             ┆ 0.5625         │\n",
      "│ financial_phrasebank           ┆ 0.007406             ┆ 0.6875         │\n",
      "│ trec                           ┆ 0.002819             ┆ 0.5            │\n",
      "│ app_reviews                    ┆ 0.002575             ┆ 0.6875         │\n",
      "│ clickbait_notclickbait_dataset ┆ 0.0023625            ┆ 0.625          │\n",
      "│ craigslist_bargains            ┆ 0.0014               ┆ 0.6875         │\n",
      "│ emotion                        ┆ 0.001088             ┆ 0.6875         │\n",
      "│ climate_fever                  ┆ 0.001088             ┆ 0.5625         │\n",
      "│ blog_authorship_corpus         ┆ 0.001025             ┆ 0.6875         │\n",
      "│ limit                          ┆ 0.000987             ┆ 0.625          │\n",
      "│ disaster_response_messages     ┆ 0.000713             ┆ 0.375          │\n",
      "│ movie_rationales               ┆ 0.000381             ┆ 0.5625         │\n",
      "└────────────────────────────────┴──────────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=-1):\n",
    "    print(\n",
    "        accuracies_grouped.group_by(\"dataset\")\n",
    "        .agg(  # across m, n, lm_type\n",
    "            pl.col(\"evaluation_bias_mean\").mean(),\n",
    "            (pl.col(\"evaluation_bias_mean\") > 0).mean().alias(\"frac_bias_gt_0\"),\n",
    "        )\n",
    "        .filter(pl.col(\"evaluation_bias_mean\") > 0)\n",
    "        .sort(\"evaluation_bias_mean\", descending=True)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
